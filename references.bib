@book{angelov2016advances,
  title     = {Advances in Computational Intelligence Systems: Contributions Presented at the 16th UK Workshop on Computational Intelligence, September 7--9, 2016, Lancaster, UK},
  author    = {Angelov, P. and Gegov, A. and Jayne, C. and Shen, Q.},
  isbn      = {9783319465623},
  series    = {Advances in Intelligent Systems and Computing},
  url       = {https://books.google.co.uk/books?id=jnD_DAAAQBAJ},
  year      = {2016},
  publisher = {Springer International Publishing}
}

@inproceedings{demirkaya2020exploring,
  author    = {Demirkaya, Ahmet and Chen, Jiasi and Oymak, Samet},
  booktitle = {2020 54th Annual Conference on Information Sciences and Systems (CISS)},
  title     = {Exploring the Role of Loss Functions in Multiclass Classification},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {1-5},
  keywords  = {Training;Deep learning;Correlation;Catalysts;Neural networks;Focusing;Mixture models;cross entropy;multiclass classification;quadratic loss;over-parameterization;deep neural networks},
  doi       = {10.1109/CISS48834.2020.1570627167}
}

download as .bib file

@article{dosovitskiy2021image,
  author     = {Alexey Dosovitskiy and
                Lucas Beyer and
                Alexander Kolesnikov and
                Dirk Weissenborn and
                Xiaohua Zhai and
                Thomas Unterthiner and
                Mostafa Dehghani and
                Matthias Minderer and
                Georg Heigold and
                Sylvain Gelly and
                Jakob Uszkoreit and
                Neil Houlsby},
  title      = {An Image is Worth 16x16 Words: Transformers for Image Recognition
                at Scale},
  journal    = {CoRR},
  volume     = {abs/2010.11929},
  year       = {2020},
  url        = {https://arxiv.org/abs/2010.11929},
  eprinttype = {arXiv},
  eprint     = {2010.11929},
  timestamp  = {Fri, 20 Nov 2020 14:04:05 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2010-11929.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@book{goodfellow2016deep,
  title     = {Deep Learning},
  author    = {Goodfellow, I. and Bengio, Y. and Courville, A.},
  isbn      = {9780262035613},
  lccn      = {2016022992},
  series    = {Adaptive Computation and Machine Learning series},
  url       = {https://books.google.co.uk/books?id=Np9SDQAAQBAJ},
  year      = {2016},
  publisher = {MIT Press}
}
@misc{kabir2024reduction,
  title         = {Reduction of Class Activation Uncertainty with Background Information},
  author        = {H M Dipu Kabir},
  year          = {2024},
  eprint        = {2305.03238},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@inproceedings{kuntal2016preprocessing,
  author    = {Pal, Kuntal Kumar and Sudeep, K. S.},
  booktitle = {2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)},
  title     = {Preprocessing for image classification by convolutional neural networks},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {1778-1781},
  keywords  = {Training;Biological neural networks;Computer architecture;Convolutional codes;Conferences;Standardization;Convolution;Preprocessing;Convolutional Neural Network;Normalization;Standardization;ZCA;CIFAR10},
  doi       = {10.1109/RTEICT.2016.7808140}
}
@inproceedings{mao2023cross,
  title     = {Cross-Entropy Loss Functions: Theoretical Analysis and Applications},
  author    = {Mao, Anqi and Mohri, Mehryar and Zhong, Yutao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  pages     = {23803--23828},
  year      = {2023},
  editor    = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume    = {202},
  series    = {Proceedings of Machine Learning Research},
  month     = {23--29 Jul},
  publisher = {PMLR},
  pdf       = {https://proceedings.mlr.press/v202/mao23b/mao23b.pdf},
  url       = {https://proceedings.mlr.press/v202/mao23b.html},
  abstract  = {Cross-entropy is a widely used loss function in applications. It coincides with the logistic loss applied to the outputs of a neural network, when the softmax is used. But, what guarantees can we rely on when using cross-entropy as a surrogate loss? We present a theoretical analysis of a broad family of loss functions, <em>comp-sum losses</em>, that includes cross-entropy (or logistic loss), generalized cross-entropy, the mean absolute error and other cross-entropy-like loss functions. We give the first $H$-consistency bounds for these loss functions. These are non-asymptotic guarantees that upper bound the zero-one loss estimation error in terms of the estimation error of a surrogate loss, for the specific hypothesis set $H$ used. We further show that our bounds are <em>tight</em>. These bounds depend on quantities called <em>minimizability gaps</em>. To make them more explicit, we give a specific analysis of these gaps for comp-sum losses. We also introduce a new family of loss functions, <em>smooth adversarial comp-sum losses</em>, that are derived from their comp-sum counterparts by adding in a related smooth term. We show that these loss functions are beneficial in the adversarial setting by proving that they admit $H$-consistency bounds. This leads to new adversarial robustness algorithms that consist of minimizing a regularized smooth adversarial comp-sum loss. While our main purpose is a theoretical analysis, we also present an extensive empirical analysis comparing comp-sum losses. We further report the results of a series of experiments demonstrating that our adversarial robustness algorithms outperform the current state-of-the-art, while also achieving a superior non-adversarial accuracy.}
}

@article{noh2021performance,
  author         = {Noh, Seol-Hyun},
  title          = {Performance Comparison of CNN Models Using Gradient Flow Analysis},
  journal        = {Informatics},
  volume         = {8},
  year           = {2021},
  number         = {3},
  article-number = {53},
  url            = {https://www.mdpi.com/2227-9709/8/3/53},
  issn           = {2227-9709},
  abstract       = {Convolutional neural networks (CNNs) are widely used among the various deep learning techniques available because of their superior performance in the fields of computer vision and natural language processing. CNNs can effectively extract the locality and correlation of input data using structures in which convolutional layers are successively applied to the input data. In general, the performance of neural networks has improved as the depth of CNNs has increased. However, an increase in the depth of a CNN is not always accompanied by an increase in the accuracy of the neural network. This is because the gradient vanishing problem may arise, causing the weights of the weighted layers to fail to converge. Accordingly, the gradient flows of the VGGNet, ResNet, SENet, and DenseNet models were analyzed and compared in this study, and the reasons for the differences in the error rate performances of the models were derived.},
  doi            = {10.3390/informatics8030053}
}
@inproceedings{park2017analysis,
  author    = {Park, Sungheon
               and Kwak, Nojun},
  editor    = {Lai, Shang-Hong
               and Lepetit, Vincent
               and Nishino, Ko
               and Sato, Yoichi},
  title     = {Analysis on the Dropout Effect in Convolutional Neural Networks},
  booktitle = {Computer Vision --  ACCV 2016},
  year      = {2017},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {189--204},
  abstract  = {Regularizing neural networks is an important task to reduce overfitting. Dropout [1] has been a widely-used regularization trick for neural networks. In convolutional neural networks (CNNs), dropout is usually applied to the fully connected layers. Meanwhile, the regularization effect of dropout in the convolutional layers has not been thoroughly analyzed in the literature. In this paper, we analyze the effect of dropout in the convolutional layers, which is indeed proved as a powerful generalization method. We observed that dropout in CNNs regularizes the networks by adding noise to the output feature maps of each layer, yielding robustness to variations of images. Based on this observation, we propose a stochastic dropout whose drop ratio varies for each iteration. Furthermore, we propose a new regularization method which is inspired by behaviors of image filters. Rather than randomly drop the activation, we selectively drop the activations which have high values across the feature map or across the channels. Experimental results validate the regularization performance of selective max-drop and stochastic dropout is competitive to the dropout or spatial dropout [2].},
  isbn      = {978-3-319-54184-6}
}

@article{smith2018discipined,
  author     = {Leslie N. Smith},
  title      = {A disciplined approach to neural network hyper-parameters: Part 1
                - learning rate, batch size, momentum, and weight decay},
  journal    = {CoRR},
  volume     = {abs/1803.09820},
  year       = {2018},
  url        = {http://arxiv.org/abs/1803.09820},
  eprinttype = {arXiv},
  eprint     = {1803.09820},
  timestamp  = {Mon, 13 Aug 2018 16:46:45 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1803-09820.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
@book{sra2012optimization,
  title     = {Optimization for Machine Learning},
  author    = {Sra, S. and Nowozin, S. and Wright, S.J.},
  isbn      = {9780262016469},
  lccn      = {2011002059},
  series    = {Neural information processing series},
  url       = {https://books.google.co.uk/books?id=JPQx7s2L1A8C},
  year      = {2012},
  publisher = {MIT Press}
}

@article{thoma2017analysis,
  author     = {Martin Thoma},
  title      = {Analysis and Optimization of Convolutional Neural Network Architectures},
  journal    = {CoRR},
  volume     = {abs/1707.09725},
  year       = {2017},
  url        = {http://arxiv.org/abs/1707.09725},
  eprinttype = {arXiv},
  eprint     = {1707.09725},
  timestamp  = {Mon, 13 Aug 2018 16:48:35 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/000117b.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}



@article{yang2018modified,
  title     = {Modified convolutional neural network based on dropout and the stochastic gradient descent optimizer},
  author    = {Yang, Jing and Yang, Guanci},
  journal   = {Algorithms},
  volume    = {11},
  number    = {3},
  pages     = {28},
  year      = {2018},
  publisher = {MDPI}
}

